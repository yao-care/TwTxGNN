#!/usr/bin/env python3
"""
æ–°èè™•ç†è…³æœ¬

åŠŸèƒ½ï¼š
1. è®€å–æ‰€æœ‰ data/news/*.json ä¾†æºæª”æ¡ˆ
2. è¼‰å…¥ keywords.json é€²è¡Œé—œéµå­—åŒ¹é…
3. è·¨ç«™å»é‡ï¼ˆç›¸ä¼¼æ¨™é¡Œåˆä½µä¾†æºï¼‰
4. è¼¸å‡º matched_news.json
5. ç”¢ç”Ÿ docs/_news/*.md é é¢
"""

import json
import re
from datetime import datetime, timezone, timedelta
from difflib import SequenceMatcher
from pathlib import Path

# å°ˆæ¡ˆæ ¹ç›®éŒ„
PROJECT_ROOT = Path(__file__).parent.parent
DATA_DIR = PROJECT_ROOT / "data" / "news"
DOCS_DIR = PROJECT_ROOT / "docs"
NEWS_COLLECTION_DIR = DOCS_DIR / "_news"

# è¨­å®š
SIMILARITY_THRESHOLD = 0.8  # æ¨™é¡Œç›¸ä¼¼åº¦é–¾å€¼
TIME_WINDOW_HOURS = 24  # å»é‡æ™‚é–“çª—å£ï¼ˆå°æ™‚ï¼‰
MAX_NEWS_AGE_DAYS = 30  # æœ€å¤§æ–°èä¿ç•™å¤©æ•¸


def load_json(path: Path) -> dict | list:
    """è¼‰å…¥ JSON æª”æ¡ˆ"""
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


def save_json(data: dict | list, path: Path):
    """å„²å­˜ JSON æª”æ¡ˆ"""
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


def load_all_sources() -> list[dict]:
    """è¼‰å…¥æ‰€æœ‰ä¾†æºçš„æ–°è"""
    all_news = []
    excluded = {"keywords.json", "matched_news.json"}

    for json_file in DATA_DIR.glob("*.json"):
        if json_file.name in excluded:
            continue

        try:
            data = load_json(json_file)
            source_name = data.get("source", json_file.stem)
            news_items = data.get("news", [])
            print(f"  - {json_file.name}: {len(news_items)} å‰‡")

            for item in news_items:
                item["_source_file"] = source_name
                all_news.append(item)

        except Exception as e:
            print(f"  è­¦å‘Š: ç„¡æ³•è¼‰å…¥ {json_file.name} - {e}")

    return all_news


def filter_old_news(news_items: list[dict]) -> list[dict]:
    """éæ¿¾è¶…é 30 å¤©çš„èˆŠæ–°è"""
    cutoff = datetime.now(timezone.utc) - timedelta(days=MAX_NEWS_AGE_DAYS)
    filtered = []

    for item in news_items:
        try:
            published = datetime.fromisoformat(item.get("published", ""))
            if published >= cutoff:
                filtered.append(item)
        except (ValueError, TypeError):
            # ç„¡æ³•è§£ææ—¥æœŸï¼Œä¿ç•™
            filtered.append(item)

    removed = len(news_items) - len(filtered)
    if removed > 0:
        print(f"  éæ¿¾èˆŠæ–°è: {removed} å‰‡")

    return filtered


def title_similarity(title1: str, title2: str) -> float:
    """è¨ˆç®—å…©å€‹æ¨™é¡Œçš„ç›¸ä¼¼åº¦"""
    # ç§»é™¤ä¾†æºæ¨™è¨˜ï¼ˆå¦‚ã€Œ- è¯åˆå ±ã€ï¼‰
    clean1 = re.sub(r"\s*[-â€“â€”]\s*[^\s]+$", "", title1).strip()
    clean2 = re.sub(r"\s*[-â€“â€”]\s*[^\s]+$", "", title2).strip()

    return SequenceMatcher(None, clean1, clean2).ratio()


def deduplicate_news(news_items: list[dict]) -> list[dict]:
    """è·¨ç«™å»é‡ï¼Œåˆä½µç›¸ä¼¼æ–°èçš„ä¾†æº"""
    # æŒ‰ç™¼å¸ƒæ™‚é–“æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
    sorted_news = sorted(
        news_items,
        key=lambda x: x.get("published", ""),
        reverse=True
    )

    merged = []
    used_indices = set()

    for i, item in enumerate(sorted_news):
        if i in used_indices:
            continue

        # æ‰¾å‡ºç›¸ä¼¼çš„æ–°è
        similar_items = [item]
        item_time = datetime.fromisoformat(
            item.get("published", datetime.now(timezone.utc).isoformat())
        )

        for j, other in enumerate(sorted_news[i + 1:], start=i + 1):
            if j in used_indices:
                continue

            # æª¢æŸ¥æ™‚é–“çª—å£
            other_time = datetime.fromisoformat(
                other.get("published", datetime.now(timezone.utc).isoformat())
            )
            time_diff = abs((item_time - other_time).total_seconds() / 3600)

            if time_diff > TIME_WINDOW_HOURS:
                continue

            # æª¢æŸ¥æ¨™é¡Œç›¸ä¼¼åº¦
            if title_similarity(item["title"], other["title"]) >= SIMILARITY_THRESHOLD:
                similar_items.append(other)
                used_indices.add(j)

        # åˆä½µä¾†æº
        all_sources = []
        seen_links = set()
        for sim_item in similar_items:
            for source in sim_item.get("sources", []):
                if source["link"] not in seen_links:
                    seen_links.add(source["link"])
                    all_sources.append(source)

        # ä½¿ç”¨æœ€æ—©çš„ç™¼å¸ƒæ™‚é–“
        earliest_time = min(
            datetime.fromisoformat(s.get("published", datetime.now(timezone.utc).isoformat()))
            for s in similar_items
        )

        merged_item = {
            "id": item["id"],
            "title": re.sub(r"\s*[-â€“â€”]\s*[^\s]+$", "", item["title"]).strip(),
            "published": earliest_time.isoformat(),
            "summary": item.get("summary", ""),
            "sources": all_sources,
            "matched_keywords": []  # ç¨å¾Œå¡«å…¥
        }
        merged.append(merged_item)
        used_indices.add(i)

    print(f"  å»é‡å¾Œ: {len(merged)} å‰‡ï¼ˆåˆä½µ {len(news_items) - len(merged)} å‰‡ï¼‰")
    return merged


def match_keywords(news_items: list[dict], keywords: dict) -> list[dict]:
    """å°æ–°èé€²è¡Œé—œéµå­—åŒ¹é…"""
    drugs = keywords.get("drugs", [])
    indications = keywords.get("indications", [])

    # å»ºç«‹è—¥ç‰© slug -> name çš„å°ç…§è¡¨
    drug_name_map = {d["slug"]: d["name"] for d in drugs}

    matched_count = 0

    for item in news_items:
        text_to_search = f"{item['title']} {item.get('summary', '')}".lower()
        matches = []

        # åŒ¹é…è—¥ç‰©
        for drug in drugs:
            drug_name = drug["name"]
            drug_slug = drug["slug"]

            # è‹±æ–‡é—œéµå­—
            for kw in drug["keywords"].get("en", []):
                if kw.lower() in text_to_search:
                    matches.append({
                        "type": "drug",
                        "slug": drug_slug,
                        "keyword": kw,
                        "name": drug_name,
                        "url": drug["url"]
                    })
                    break  # åŒä¸€è—¥ç‰©åªè¨˜éŒ„ä¸€æ¬¡

            # ä¸­æ–‡é—œéµå­—
            for kw in drug["keywords"].get("zh", []):
                if kw in item["title"] or kw in item.get("summary", ""):
                    # ç¢ºä¿æ²’æœ‰é‡è¤‡
                    if not any(m["slug"] == drug_slug for m in matches):
                        matches.append({
                            "type": "drug",
                            "slug": drug_slug,
                            "keyword": kw,
                            "name": drug_name,
                            "url": drug["url"]
                        })
                    break

        # åŒ¹é…é©æ‡‰ç—‡
        for ind in indications:
            ind_name = ind["name"]

            # å°‡ related_drugs å¾ slug è½‰æ›ç‚º {slug, name} æ ¼å¼
            related_drugs = [
                {"slug": slug, "name": drug_name_map.get(slug, slug)}
                for slug in ind.get("related_drugs", [])
            ]

            # è‹±æ–‡é—œéµå­—
            for kw in ind["keywords"].get("en", []):
                if kw.lower() in text_to_search:
                    matches.append({
                        "type": "indication",
                        "name": ind_name,
                        "keyword": kw,
                        "related_drugs": related_drugs
                    })
                    break

            # ä¸­æ–‡é—œéµå­—
            for kw in ind["keywords"].get("zh", []):
                if kw in item["title"] or kw in item.get("summary", ""):
                    # ç¢ºä¿åŒä¸€é—œéµå­—æ²’æœ‰é‡è¤‡ï¼ˆé¿å… "æ„Ÿå†’" åŒæ™‚å‡ºç¾åœ¨ "common cold" å’Œ "æ„Ÿå†’" å…©å€‹æ¢ç›®ï¼‰
                    if not any(m.get("keyword") == kw and m["type"] == "indication" for m in matches):
                        matches.append({
                            "type": "indication",
                            "name": ind_name,
                            "keyword": kw,
                            "related_drugs": related_drugs
                        })
                    break

        item["matched_keywords"] = matches
        if matches:
            matched_count += 1

    print(f"  åŒ¹é…åˆ°é—œéµå­—: {matched_count} å‰‡")
    return news_items


def generate_news_pages(matched_news: list[dict], keywords: dict):
    """ç”¢ç”Ÿ Jekyll æ–°èé é¢"""
    # ç¢ºä¿ç›®éŒ„å­˜åœ¨
    NEWS_COLLECTION_DIR.mkdir(parents=True, exist_ok=True)

    # æ¸…é™¤èˆŠé é¢
    for old_file in NEWS_COLLECTION_DIR.glob("*.md"):
        old_file.unlink()

    # è¼‰å…¥å®Œæ•´çš„è—¥ç‰©è³‡æ–™ï¼ˆåŒ…å« original_indication ç­‰ï¼‰
    drugs_data_path = DOCS_DIR / "data" / "drugs.json"
    drugs_data = load_json(drugs_data_path) if drugs_data_path.exists() else {"drugs": []}
    drugs_detail_map = {d["slug"]: d for d in drugs_data.get("drugs", [])}

    # è¼‰å…¥æœå°‹ç´¢å¼•ï¼ˆåŒ…å«å®Œæ•´çš„é æ¸¬é©æ‡‰ç—‡åˆ—è¡¨ï¼‰
    search_index_path = DOCS_DIR / "data" / "search-index.json"
    search_index = load_json(search_index_path) if search_index_path.exists() else {"drugs": []}
    search_index_map = {d["slug"]: d for d in search_index.get("drugs", [])}

    # å»ºç«‹è—¥ç‰©å’Œé©æ‡‰ç—‡çš„æ–°èç´¢å¼•
    drug_news = {}  # slug -> [news_items]
    indication_news = {}  # name -> [news_items]

    for item in matched_news:
        if not item.get("matched_keywords"):
            continue

        for match in item["matched_keywords"]:
            if match["type"] == "drug":
                slug = match["slug"]
                if slug not in drug_news:
                    drug_news[slug] = []
                drug_news[slug].append(item)
            elif match["type"] == "indication":
                name = match["name"]
                if name not in indication_news:
                    indication_news[name] = []
                indication_news[name].append(item)
                # ä¹ŸæŠŠæ–°èåŠ åˆ°ç›¸é—œè—¥ç‰©
                for related_drug in match.get("related_drugs", []):
                    slug = related_drug.get("slug")
                    if slug:
                        if slug not in drug_news:
                            drug_news[slug] = []
                        # é¿å…é‡è¤‡åŠ å…¥åŒä¸€å‰‡æ–°è
                        if item not in drug_news[slug]:
                            drug_news[slug].append(item)

    # ç”¢ç”Ÿæ‰€æœ‰è—¥ç‰©æ–°èé é¢ï¼ˆ191 å€‹å…¨éƒ¨ç”Ÿæˆï¼‰
    drugs_map = {d["slug"]: d for d in keywords.get("drugs", [])}
    drug_count = 0

    for drug in keywords.get("drugs", []):
        slug = drug["slug"]
        drug_info = drugs_map.get(slug, {})
        drug_detail = drugs_detail_map.get(slug, {})
        drug_search = search_index_map.get(slug, {})
        news_items = drug_news.get(slug, [])  # å¯èƒ½ç‚ºç©º
        generate_drug_news_page(slug, drug_info, drug_detail, drug_search, news_items)
        drug_count += 1

    # ç”¢ç”Ÿé©æ‡‰ç—‡æ–°èé é¢ï¼ˆåªæœ‰åŒ¹é…åˆ°æ–°èçš„ï¼‰
    for name, items in indication_news.items():
        generate_indication_news_page(name, items, keywords)

    print(f"  ç”¢ç”Ÿé é¢: {drug_count} è—¥ç‰© + {len(indication_news)} é©æ‡‰ç—‡")


def slugify(text: str) -> str:
    """å°‡æ–‡å­—è½‰æ›ç‚º URL-safe çš„ slug"""
    # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œä¿ç•™è‹±æ–‡å­—æ¯ã€æ•¸å­—å’Œé€£å­—è™Ÿ
    slug = re.sub(r"[^\w\s-]", "", text.lower())
    slug = re.sub(r"[\s_]+", "-", slug)
    return slug.strip("-")


def generate_drug_news_page(slug: str, drug_info: dict, drug_detail: dict, drug_search: dict, news_items: list[dict]):
    """ç”¢ç”Ÿè—¥ç‰©æ–°èé é¢"""
    name = drug_info.get("name", slug)
    original_indication = drug_detail.get("original_indication", "")
    evidence_level = drug_detail.get("evidence_level", "")
    indications = drug_search.get("indications", [])

    # æ”¶é›†æœ‰æ–°èçš„é©æ‡‰ç—‡ï¼ˆè‹±æ–‡å -> ä¸­æ–‡é—œéµå­—ï¼‰
    matched_indications = {}
    for item in news_items:
        for match in item.get("matched_keywords", []):
            if match.get("type") == "indication":
                en_name = match.get("name", "")
                zh_keyword = match.get("keyword", en_name)
                matched_indications[en_name] = zh_keyword

    content = f"""---
layout: default
title: "{name} ç›¸é—œæ–°è"
parent: å¥åº·æ–°è
nav_exclude: true
permalink: /news/{slug}/
---

# {name} ç›¸é—œæ–°è

[â† è¿”å›æ–°èç¸½è¦½]({{{{ '/news/' | relative_url }}}})

---

<div class="drug-info-card">
<strong>è—¥ç‰©è³‡è¨Š</strong>
<ul>
"""

    if original_indication:
        content += f"<li><strong>åŸé©æ‡‰ç—‡</strong>ï¼š{original_indication}</li>\n"
    if evidence_level:
        content += f"<li><strong>è­‰æ“šç­‰ç´š</strong>ï¼š{evidence_level}</li>\n"

    # åˆ—å‡ºé æ¸¬é©æ‡‰ç—‡ï¼ˆæœ‰æ–°èçš„æ¨™è¨˜é¡è‰²ï¼‰
    if indications:
        content += f"<li><strong>é æ¸¬é©æ‡‰ç—‡</strong>ï¼ˆ{len(indications)} å€‹ï¼‰ï¼š<ul>\n"
        for ind in indications:
            ind_name = ind.get("name", "")
            ind_score = ind.get("score", 0)
            if ind_name in matched_indications:
                zh_keyword = matched_indications[ind_name]
                content += f'<li class="indication-matched">{ind_name}ï¼ˆ{ind_score:.1f}%ï¼‰<span class="indication-tag">ğŸ“° {zh_keyword}</span></li>\n'
            else:
                content += f"<li>{ind_name}ï¼ˆ{ind_score:.1f}%ï¼‰</li>\n"
        content += "</ul></li>\n"

    content += f"""</ul>
<p><a href="{{{{ '/drugs/{slug}/' | relative_url }}}}">æŸ¥çœ‹å®Œæ•´è—¥ç‰©å ±å‘Š â†’</a></p>
</div>

## ç›¸é—œæ–°èï¼ˆ{len(news_items)} å‰‡ï¼‰

"""

    if news_items:
        for item in sorted(news_items, key=lambda x: x["published"], reverse=True):
            # æ ¼å¼åŒ–æ—¥æœŸ
            try:
                dt = datetime.fromisoformat(item["published"])
                date_str = dt.strftime("%Y-%m-%d")
            except (ValueError, TypeError):
                date_str = "æœªçŸ¥æ—¥æœŸ"

            # ä¾†æºé€£çµ
            sources = item.get("sources", [])
            first_link = sources[0]["link"] if sources else "#"
            sources_html = " Â· ".join(
                f'[{s["name"]}]({s["link"]})'
                for s in sources
            )

            # åŒ¹é…çš„é—œéµå­—æ¨™ç±¤
            keyword_tags = []
            for match in item.get("matched_keywords", []):
                if match.get("type") == "indication":
                    zh_keyword = match.get("keyword", match.get("name", ""))
                    keyword_tags.append(f'<span class="news-indication-tag">{zh_keyword}</span>')
                elif match.get("type") == "drug":
                    keyword_tags.append(f'<span class="news-drug-tag">{match.get("name", "")}</span>')
            keyword_html = " ".join(keyword_tags) if keyword_tags else ""

            content += f"""### [{item["title"]}]({first_link})

{date_str} {keyword_html}

ä¾†æºï¼š{sources_html}

---

"""
    else:
        content += """*ç›®å‰æ²’æœ‰ç›¸é—œæ–°èå ±å°ã€‚ç•¶æœ‰æ–°èæåˆ°æ­¤è—¥ç‰©æ™‚ï¼Œç³»çµ±æœƒè‡ªå‹•æ”¶é›†ä¸¦é¡¯ç¤ºåœ¨é€™è£¡ã€‚*

"""

    content += """
<div class="disclaimer">
<strong>å…è²¬è²æ˜</strong>ï¼šæœ¬é æ–°èç”±ç³»çµ±è‡ªå‹•æ”¶é›†ï¼Œåƒ…ä¾›ç ”ç©¶åƒè€ƒï¼Œä¸æ§‹æˆé†«ç™‚å»ºè­°ã€‚
</div>

<style>
.indication-matched {
  background: #fff3e0;
  padding: 4px 8px;
  border-radius: 4px;
  border-left: 3px solid #ff9800;
}
.indication-tag {
  display: inline-block;
  background: #ff9800;
  color: white;
  padding: 2px 8px;
  border-radius: 12px;
  font-size: 0.8em;
  margin-left: 8px;
}
.news-indication-tag {
  display: inline-block;
  background: #ff9800;
  color: white;
  padding: 2px 10px;
  border-radius: 12px;
  font-size: 0.85em;
  margin-left: 4px;
}
.news-drug-tag {
  display: inline-block;
  background: #1565c0;
  color: white;
  padding: 2px 10px;
  border-radius: 12px;
  font-size: 0.85em;
  margin-left: 4px;
}
</style>
"""

    # å¯«å…¥æª”æ¡ˆ
    output_path = NEWS_COLLECTION_DIR / f"{slug}.md"
    with open(output_path, "w", encoding="utf-8") as f:
        f.write(content)


def generate_indication_news_page(name: str, news_items: list[dict], keywords: dict):
    """ç”¢ç”Ÿé©æ‡‰ç—‡æ–°èé é¢"""
    slug = slugify(name)

    # å¾æ–°èé …ç›®ä¸­æ‰¾å‡ºä¸­æ–‡é—œéµå­—
    zh_keyword = name  # é è¨­ä½¿ç”¨è‹±æ–‡å
    for item in news_items:
        for match in item.get("matched_keywords", []):
            if match.get("type") == "indication" and match.get("name") == name:
                if match.get("keyword"):
                    zh_keyword = match["keyword"]
                    break
        if zh_keyword != name:
            break

    # æ‰¾å‡ºç›¸é—œè—¥ç‰©
    related_drugs = set()
    for ind in keywords.get("indications", []):
        if ind["name"] == name:
            related_drugs.update(ind.get("related_drugs", []))
            break

    drugs_map = {d["slug"]: d for d in keywords.get("drugs", [])}

    # æ¨™é¡Œï¼šå„ªå…ˆä½¿ç”¨ä¸­æ–‡ï¼Œæ‹¬è™Ÿå…§é¡¯ç¤ºè‹±æ–‡
    display_title = f"{zh_keyword}ï¼ˆ{name}ï¼‰" if zh_keyword != name else name

    content = f"""---
layout: default
title: "{display_title} ç›¸é—œæ–°è"
parent: å¥åº·æ–°è
nav_exclude: true
permalink: /news/{slug}/
---

# {display_title} ç›¸é—œæ–°è

[â† è¿”å›æ–°èç¸½è¦½]({{{{ '/news/' | relative_url }}}})

---

"""

    if related_drugs:
        content += """<div class="related-drugs-card">
<strong>ç›¸é—œè—¥ç‰©å ±å‘Š</strong>
<p>ä»¥ä¸‹è—¥ç‰©çš„é æ¸¬é©æ‡‰ç—‡å¯èƒ½èˆ‡æ­¤ç–¾ç—…ç›¸é—œï¼š</p>
<ul>
"""
        for drug_slug in sorted(related_drugs):
            drug = drugs_map.get(drug_slug, {})
            drug_name = drug.get("name", drug_slug)
            content += f'<li><a href="{{{{ \'/drugs/{drug_slug}/\' | relative_url }}}}">{drug_name}</a></li>\n'

        content += "</ul>\n</div>\n\n"

    content += f"""## ç›¸é—œæ–°èï¼ˆ{len(news_items)} å‰‡ï¼‰

"""

    for item in sorted(news_items, key=lambda x: x["published"], reverse=True):
        try:
            dt = datetime.fromisoformat(item["published"])
            date_str = dt.strftime("%Y-%m-%d")
        except (ValueError, TypeError):
            date_str = "æœªçŸ¥æ—¥æœŸ"

        sources = item.get("sources", [])
        first_link = sources[0]["link"] if sources else "#"
        sources_html = " Â· ".join(
            f'[{s["name"]}]({s["link"]})'
            for s in sources
        )

        content += f"""### [{item["title"]}]({first_link})

{date_str}

ä¾†æºï¼š{sources_html}

---

"""

    content += """
<div class="disclaimer">
<strong>å…è²¬è²æ˜</strong>ï¼šæœ¬é æ–°èç”±ç³»çµ±è‡ªå‹•æ”¶é›†ï¼Œåƒ…ä¾›ç ”ç©¶åƒè€ƒï¼Œä¸æ§‹æˆé†«ç™‚å»ºè­°ã€‚
</div>
"""

    output_path = NEWS_COLLECTION_DIR / f"{slug}.md"
    with open(output_path, "w", encoding="utf-8") as f:
        f.write(content)


def generate_news_index(matched_news: list[dict]):
    """ç”¢ç”Ÿå‰ç«¯ç”¨çš„æ–°èç´¢å¼• JSON"""
    # åªä¿ç•™æœ‰åŒ¹é…é—œéµå­—çš„æ–°è
    indexed_news = [
        {
            "id": item["id"],
            "title": item["title"],
            "published": item["published"],
            "sources": item["sources"],
            "keywords": item["matched_keywords"]
        }
        for item in matched_news
        if item.get("matched_keywords")
    ]

    output = {
        "generated": datetime.now(timezone.utc).isoformat(),
        "count": len(indexed_news),
        "news": indexed_news
    }

    output_path = DOCS_DIR / "data" / "news-index.json"
    output_path.parent.mkdir(parents=True, exist_ok=True)
    save_json(output, output_path)
    print(f"  ç”¢ç”Ÿç´¢å¼•: {output_path}")


def main():
    print("è™•ç†æ–°èè³‡æ–™...")

    # 1. è¼‰å…¥æ‰€æœ‰ä¾†æº
    print("\nè¼‰å…¥ä¾†æºæª”æ¡ˆ:")
    all_news = load_all_sources()
    print(f"  ç¸½è¨ˆ: {len(all_news)} å‰‡")

    # 2. éæ¿¾èˆŠæ–°è
    print("\néæ¿¾èˆŠæ–°è:")
    all_news = filter_old_news(all_news)

    # 3. å»é‡
    print("\nè·¨ç«™å»é‡:")
    all_news = deduplicate_news(all_news)

    # 4. è¼‰å…¥é—œéµå­—ä¸¦åŒ¹é…
    print("\né—œéµå­—åŒ¹é…:")
    keywords = load_json(DATA_DIR / "keywords.json")
    print(f"  é—œéµå­—: {keywords['drug_count']} è—¥ç‰© + {keywords['indication_count']} é©æ‡‰ç—‡")
    all_news = match_keywords(all_news, keywords)

    # 5. è¼¸å‡º matched_news.json
    output = {
        "last_updated": datetime.now(timezone.utc).isoformat(),
        "total_count": len(all_news),
        "matched_count": sum(1 for n in all_news if n.get("matched_keywords")),
        "news": all_news
    }
    save_json(output, DATA_DIR / "matched_news.json")
    print(f"\nè¼¸å‡º: {DATA_DIR / 'matched_news.json'}")

    # 6. ç”¢ç”Ÿé é¢
    print("\nç”¢ç”Ÿ Jekyll é é¢:")
    generate_news_pages(all_news, keywords)

    # 7. ç”¢ç”Ÿç´¢å¼•
    generate_news_index(all_news)

    print("\nå®Œæˆï¼")


if __name__ == "__main__":
    main()
